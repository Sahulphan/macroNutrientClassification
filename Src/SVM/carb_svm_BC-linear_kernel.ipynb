{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import Series, DataFrame\n",
    "from pylab import rcParams\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "sb.set(style=\"white\")\n",
    "sb.set(style=\"whitegrid\", color_codes=True)\n",
    "import itertools\n",
    "from sklearn import svm\n",
    "import graphviz\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold, LeaveOneOut\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 13)\n",
      "['ID', 'TruncatedID', 'Mean', 'SD', 'AUC', 'Peak', 'MaxIndex', 'Halfpk', 'Skew', 'HHMean', 'Fat', 'Carb', 'Protein']\n"
     ]
    }
   ],
   "source": [
    "### reading data and printing the data ###\n",
    "data = pd.read_csv('/home/sahulphaniraj/csce633-ml/MachineLearningProject/Dataset/classical_feature.csv',names=['ID','TruncatedID','Mean','SD','AUC','Peak','MaxIndex','Halfpk','Skew','HHMean','Fat','Carb','Protein'\n",
    "])\n",
    "print(data.shape)\n",
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51 48 54]\n",
      "['2133-002' '2133-022' '2133-020' '2133-004' '2133-010' '2133-015'\n",
      " '2133-040' '2133-013' '2133-008' '2133-019' '2133-001' '2133-024'\n",
      " '2133-011' '2133-018' '2133-006' '2133-032' '2133-025' '2133-017'\n",
      " '2133-028' '2133-039' '2133-021' '2133-030' '2133-033' '2133-037'\n",
      " '2133-026' '2133-012' '2133-036' '2133-035' '2133-041' '2133-009']\n"
     ]
    }
   ],
   "source": [
    "### CHO\n",
    "CHO_data = data.drop(['ID','Protein','Fat'], axis=1) #taking only the Carb target column\n",
    "print(CHO_data.Carb.unique())# Finding the unique values in that column\n",
    "print(CHO_data.TruncatedID.unique()) #finiding unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold : 1\n",
      "1\n",
      "For fold : 2\n",
      "1\n",
      "For fold : 3\n",
      "1\n",
      "For fold : 4\n",
      "7\n",
      "For fold : 5\n",
      "6\n",
      "For fold : 6\n",
      "14\n",
      "For fold : 7\n",
      "1\n",
      "For fold : 8\n",
      "2\n",
      "For fold : 9\n",
      "1\n",
      "For fold : 10\n",
      "3\n",
      "For fold : 11\n",
      "2\n",
      "For fold : 12\n",
      "1\n",
      "For fold : 13\n",
      "1\n",
      "For fold : 14\n",
      "3\n",
      "For fold : 15\n",
      "5\n",
      "For fold : 16\n",
      "1\n",
      "For fold : 17\n",
      "4\n",
      "For fold : 18\n",
      "1\n",
      "For fold : 19\n",
      "10\n",
      "For fold : 20\n",
      "1\n",
      "For fold : 21\n",
      "13\n",
      "For fold : 22\n",
      "1\n",
      "For fold : 23\n",
      "1\n",
      "For fold : 24\n",
      "2\n",
      "For fold : 25\n",
      "15\n",
      "For fold : 26\n",
      "18\n",
      "For fold : 27\n",
      "1\n",
      "For fold : 28\n",
      "5\n",
      "For fold : 29\n",
      "2\n",
      "For fold : 30\n",
      "1\n",
      "The final accuracy is :  0.6666666666666666\n",
      "The average F1-score is :  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "###for LOW-HIGH\n",
    "\n",
    "CHO_LH_data = CHO_data[CHO_data['Carb'].isin([48,54])] #considering the two classes for binary classification\n",
    "CHO_LH_data.loc[CHO_LH_data.Carb == 48, 'CHO_bin'] = 0 #giving them binary values, as double variables sometimes create error\n",
    "CHO_LH_data.loc[CHO_LH_data.Carb== 54, 'CHO_bin'] = 1\n",
    "CHO_LH_data = CHO_LH_data.drop(['Carb'], axis=1)\n",
    "#the above 4 lines will be different for different classification, the remaining code will be same\n",
    "X = CHO_LH_data.loc[:, CHO_LH_data.columns != 'CHO_bin'] #Feature set\n",
    "Y = CHO_LH_data.loc[:, CHO_LH_data.columns == 'CHO_bin'] #target value/label\n",
    "\n",
    "values =['2133-002', '2133-022', '2133-020', '2133-004', '2133-010', '2133-015',\n",
    " '2133-040' ,'2133-013' ,'2133-008' ,'2133-019' ,'2133-001', '2133-024',\n",
    " '2133-011' ,'2133-018', '2133-006', '2133-032' ,'2133-025', '2133-017',\n",
    " '2133-028' ,'2133-039', '2133-021', '2133-030', '2133-033', '2133-037',\n",
    " '2133-026', '2133-012' ,'2133-036', '2133-035', '2133-041' ,'2133-009'] #needed for manual loss\n",
    "\n",
    "final_accuracy =[] #will be used for finding mean across nested cross validation\n",
    "F1score = []\n",
    "\n",
    "for i in range(0,30): #as we have 30 IDs\n",
    "    print(\"For fold :\", i+1) #will print the current fold number, this is for initial check, can delete it\n",
    "    test =CHO_LH_data[CHO_LH_data['TruncatedID'].isin([values[i]])] #test data    \n",
    "    train =CHO_LH_data[~CHO_LH_data['TruncatedID'].isin([values[i]])]#train data\n",
    "    X_train = train.loc[:, train.columns != 'CHO_bin'] #test feature\n",
    "    Y_train = train.loc[:, train.columns == 'CHO_bin'] #test label\n",
    "    X_test = test.loc[:, test.columns != 'CHO_bin']#train feature    \n",
    "    Y_test =test.loc[:, test.columns == 'CHO_bin']#train label\n",
    "    \n",
    "    train_ids = X_train['TruncatedID']#needed for inner cross-validation\n",
    "    train_ids = np.array(train_ids)#needed for inner cross-validation\n",
    "    train_ids = np.unique(train_ids)#needed for inner cross-validation\n",
    "    X_train = X_train.drop(['TruncatedID'],axis = 1) #dropping this column as it is not a feature for classification\n",
    "    X_test = X_test.drop(['TruncatedID'],axis = 1)#dropping this column as it is not a feature for classification\n",
    "    max_acc = 0 #initializing maximum accuracy which will be needed later    \n",
    "    ypred_all = []\n",
    "    for C_val in range(1,20): #ranges of decision tree depth\n",
    "        ypred_all = []\n",
    "        for j in range(0,29): #as inner cross validation ha 29 IDs\n",
    "            inner_test = train[train['TruncatedID'].isin([train_ids[j]])] #validation set\n",
    "            inner_train = train[~train['TruncatedID'].isin([train_ids[j]])]#inner train set\n",
    "            xtr = inner_train.loc[:, inner_train.columns != 'CHO_bin'] #inner train set features\n",
    "            ytr = inner_train.loc[:, inner_train.columns == 'CHO_bin']# inner train set label\n",
    "            xvl = inner_test.loc[:, inner_test.columns != 'CHO_bin']#validation set features\n",
    "            yvl = inner_test.loc[:, inner_test.columns == 'CHO_bin']#validation set labels\n",
    "            xtr = xtr.drop(['TruncatedID'],axis = 1)#We dont need this column for classification        \n",
    "            xvl = xvl.drop(['TruncatedID'],axis = 1)#We dont need this column for classification\n",
    "            clf = svm.SVC(C=C_val,kernel='linear') #SVM classifier\n",
    "            #convert dataframe to array\n",
    "            #xtr_arr = xtr.as_matrix(columns=None)\n",
    "            #ytr_arr = ytr.as_matrix(columns=None)\n",
    "            #clf = clf.fit(xtr_arr,ytr_arr)\n",
    "            clf=clf.fit(xtr,ytr)\n",
    "            ypred = clf.predict(xvl)#prediction\n",
    "            ypred_all.append(accuracy_score(yvl,ypred))\n",
    "        ypred_mean = np.mean(ypred_all)#finding mean of all accuracy\n",
    "        if max_acc<ypred_mean:#finding the depth which gives maximum accuracy\n",
    "            max_acc = ypred_mean\n",
    "            best_param = C_val\n",
    "    #best SVM classifier\n",
    "    print(best_param)\n",
    "    clf = svm.SVC(C=best_param,kernel='linear') \n",
    "    clf=clf.fit(X_train,Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    final_accuracy.append(accuracy_score(Y_test,Y_pred))\n",
    "    dic = classification_report(Y_test,Y_pred,output_dict=True)\n",
    "    dic2 = dic['macro avg']\n",
    "    F1score.append(dic2['f1-score'])\n",
    "print(\"The final accuracy is : \", np.mean(final_accuracy))#final result (mean of accuracies in outer CV)\n",
    "print(\"The average F1-score is : \", np.mean(F1score))#final result (mean of accuracies in outer CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold : 1\n",
      "12\n",
      "For fold : 2\n",
      "3\n",
      "For fold : 3\n",
      "12\n",
      "For fold : 4\n",
      "18\n",
      "For fold : 5\n",
      "12\n",
      "For fold : 6\n",
      "3\n",
      "For fold : 7\n",
      "5\n",
      "For fold : 8\n",
      "4\n",
      "For fold : 9\n",
      "12\n",
      "For fold : 10\n",
      "10\n",
      "For fold : 11\n",
      "5\n",
      "For fold : 12\n",
      "3\n",
      "For fold : 13\n",
      "5\n",
      "For fold : 14\n",
      "8\n",
      "For fold : 15\n",
      "5\n",
      "For fold : 16\n",
      "5\n",
      "For fold : 17\n",
      "10\n",
      "For fold : 18\n",
      "3\n",
      "For fold : 19\n",
      "16\n",
      "For fold : 20\n",
      "12\n",
      "For fold : 21\n",
      "14\n",
      "For fold : 22\n",
      "7\n",
      "For fold : 23\n",
      "6\n",
      "For fold : 24\n",
      "11\n",
      "For fold : 25\n",
      "5\n",
      "For fold : 26\n",
      "2\n",
      "For fold : 27\n",
      "9\n",
      "For fold : 28\n",
      "6\n",
      "For fold : 29\n",
      "7\n",
      "For fold : 30\n",
      "4\n",
      "The final accuracy is :  0.775\n",
      "The average F1-score is :  0.775\n"
     ]
    }
   ],
   "source": [
    "#MEDIUM-HIGH\n",
    "\n",
    "CHO_LH_data = CHO_data[CHO_data['Carb'].isin([51,54])] #considering the two classes for binary classification\n",
    "CHO_LH_data.loc[CHO_LH_data.Carb == 51, 'CHO_bin'] = 0 #giving them binary values, as double variables sometimes create error\n",
    "CHO_LH_data.loc[CHO_LH_data.Carb== 54, 'CHO_bin'] = 1\n",
    "CHO_LH_data = CHO_LH_data.drop(['Carb'], axis=1)\n",
    "#the above 4 lines will be different for different classification, the remaining code will be same\n",
    "X = CHO_LH_data.loc[:, CHO_LH_data.columns != 'CHO_bin'] #Feature set\n",
    "Y = CHO_LH_data.loc[:, CHO_LH_data.columns == 'CHO_bin'] #target value/label\n",
    "\n",
    "values =['2133-002', '2133-022', '2133-020', '2133-004', '2133-010', '2133-015',\n",
    " '2133-040' ,'2133-013' ,'2133-008' ,'2133-019' ,'2133-001', '2133-024',\n",
    " '2133-011' ,'2133-018', '2133-006', '2133-032' ,'2133-025', '2133-017',\n",
    " '2133-028' ,'2133-039', '2133-021', '2133-030', '2133-033', '2133-037',\n",
    " '2133-026', '2133-012' ,'2133-036', '2133-035', '2133-041' ,'2133-009'] #needed for manual loss\n",
    "\n",
    "final_accuracy =[] #will be used for finding mean across nested cross validation\n",
    "F1score = []\n",
    "\n",
    "for i in range(0,30): #as we have 30 IDs\n",
    "    print(\"For fold :\", i+1) #will print the current fold number, this is for initial check, can delete it\n",
    "    test =CHO_LH_data[CHO_LH_data['TruncatedID'].isin([values[i]])] #test data    \n",
    "    train =CHO_LH_data[~CHO_LH_data['TruncatedID'].isin([values[i]])]#train data\n",
    "    X_train = train.loc[:, train.columns != 'CHO_bin'] #test feature\n",
    "    Y_train = train.loc[:, train.columns == 'CHO_bin'] #test label\n",
    "    X_test = test.loc[:, test.columns != 'CHO_bin']#train feature    \n",
    "    Y_test =test.loc[:, test.columns == 'CHO_bin']#train label\n",
    "    \n",
    "    train_ids = X_train['TruncatedID']#needed for inner cross-validation\n",
    "    train_ids = np.array(train_ids)#needed for inner cross-validation\n",
    "    train_ids = np.unique(train_ids)#needed for inner cross-validation\n",
    "    X_train = X_train.drop(['TruncatedID'],axis = 1) #dropping this column as it is not a feature for classification\n",
    "    X_test = X_test.drop(['TruncatedID'],axis = 1)#dropping this column as it is not a feature for classification\n",
    "    max_acc = 0 #initializing maximum accuracy which will be needed later    \n",
    "    ypred_all = []\n",
    "    for C_val in range(1,20): #ranges of decision tree depth\n",
    "        ypred_all = []\n",
    "        for j in range(0,29): #as inner cross validation ha 29 IDs\n",
    "            inner_test = train[train['TruncatedID'].isin([train_ids[j]])] #validation set\n",
    "            inner_train = train[~train['TruncatedID'].isin([train_ids[j]])]#inner train set\n",
    "            xtr = inner_train.loc[:, inner_train.columns != 'CHO_bin'] #inner train set features\n",
    "            ytr = inner_train.loc[:, inner_train.columns == 'CHO_bin']# inner train set label\n",
    "            xvl = inner_test.loc[:, inner_test.columns != 'CHO_bin']#validation set features\n",
    "            yvl = inner_test.loc[:, inner_test.columns == 'CHO_bin']#validation set labels\n",
    "            xtr = xtr.drop(['TruncatedID'],axis = 1)#We dont need this column for classification        \n",
    "            xvl = xvl.drop(['TruncatedID'],axis = 1)#We dont need this column for classification\n",
    "            clf = svm.SVC(C=C_val,kernel = 'linear') #SVM classifier\n",
    "            #convert dataframe to array\n",
    "            #xtr_arr = xtr.as_matrix(columns=None)\n",
    "            #ytr_arr = ytr.as_matrix(columns=None)\n",
    "            #clf = clf.fit(xtr_arr,ytr_arr)\n",
    "            clf=clf.fit(xtr,ytr)\n",
    "            ypred = clf.predict(xvl)#prediction\n",
    "            ypred_all.append(accuracy_score(yvl,ypred))\n",
    "        ypred_mean = np.mean(ypred_all)#finding mean of all accuracy\n",
    "        if max_acc<ypred_mean:#finding the depth which gives maximum accuracy\n",
    "            max_acc = ypred_mean\n",
    "            best_param = C_val\n",
    "    #best SVM classifier\n",
    "    print(best_param)\n",
    "    clf = svm.SVC(C=best_param, kernel='linear') \n",
    "    clf=clf.fit(X_train,Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    final_accuracy.append(accuracy_score(Y_test,Y_pred))\n",
    "    dic = classification_report(Y_test,Y_pred,output_dict=True)\n",
    "    dic2 = dic['micro avg']\n",
    "    F1score.append(dic2['f1-score'])\n",
    "print(\"The final accuracy is : \", np.mean(final_accuracy))#final result (mean of accuracies in outer CV)\n",
    "print(\"The average F1-score is : \", np.mean(F1score))#final result (mean of accuracies in outer CV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold : 1\n",
      "1\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'micro avg': {'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1-score': 0.6666666666666666, 'support': 3}, 'macro avg': {'precision': 0.3333333333333333, 'recall': 0.5, 'f1-score': 0.4, 'support': 3}, 'weighted avg': {'precision': 0.4444444444444444, 'recall': 0.6666666666666666, 'f1-score': 0.5333333333333333, 'support': 3}}\n",
      "For fold : 2\n",
      "14\n",
      "{'0.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, '1.0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}}\n",
      "For fold : 3\n",
      "1\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}}\n",
      "For fold : 4\n",
      "2\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}}\n",
      "For fold : 5\n",
      "16\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 6\n",
      "8\n",
      "{'0.0': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, '1.0': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, 'micro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'macro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'weighted avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}}\n",
      "For fold : 7\n",
      "12\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 8\n",
      "1\n",
      "{'0.0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 2}, '1.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}}\n",
      "For fold : 9\n",
      "10\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}}\n",
      "For fold : 10\n",
      "2\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}}\n",
      "For fold : 11\n",
      "1\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 12\n",
      "1\n",
      "{'0.0': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, '1.0': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, 'micro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'macro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'weighted avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}}\n",
      "For fold : 13\n",
      "1\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 14\n",
      "1\n",
      "{'0.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, '1.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, 'micro avg': {'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1-score': 0.6666666666666666, 'support': 3}, 'macro avg': {'precision': 0.3333333333333333, 'recall': 0.5, 'f1-score': 0.4, 'support': 3}, 'weighted avg': {'precision': 0.4444444444444444, 'recall': 0.6666666666666666, 'f1-score': 0.5333333333333333, 'support': 3}}\n",
      "For fold : 15\n",
      "2\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 16\n",
      "2\n",
      "{'0.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, '1.0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}}\n",
      "For fold : 17\n",
      "1\n",
      "{'0.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, '1.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 18\n",
      "7\n",
      "{'0.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, '1.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3}, 'macro avg': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3}, 'weighted avg': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3}}\n",
      "For fold : 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 20\n",
      "13\n",
      "{'0.0': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, '1.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'micro avg': {'precision': 0.3333333333333333, 'recall': 0.3333333333333333, 'f1-score': 0.3333333333333333, 'support': 3}, 'macro avg': {'precision': 0.25, 'recall': 0.25, 'f1-score': 0.25, 'support': 3}, 'weighted avg': {'precision': 0.3333333333333333, 'recall': 0.3333333333333333, 'f1-score': 0.3333333333333333, 'support': 3}}\n",
      "For fold : 21\n",
      "1\n",
      "{'0.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, '1.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 22\n",
      "6\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 23\n",
      "11\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}}\n",
      "For fold : 24\n",
      "1\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 25\n",
      "1\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'micro avg': {'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1-score': 0.6666666666666666, 'support': 3}, 'macro avg': {'precision': 0.3333333333333333, 'recall': 0.5, 'f1-score': 0.4, 'support': 3}, 'weighted avg': {'precision': 0.4444444444444444, 'recall': 0.6666666666666666, 'f1-score': 0.5333333333333333, 'support': 3}}\n",
      "For fold : 26\n",
      "2\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'micro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3}, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3}}\n",
      "For fold : 27\n",
      "1\n",
      "{'0.0': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.75, 'recall': 0.75, 'f1-score': 0.75, 'support': 4}, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "For fold : 28\n",
      "1\n",
      "{'0.0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 2}, '1.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}}\n",
      "For fold : 29\n",
      "3\n",
      "{'0.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, '1.0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1}, 'micro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2}}\n",
      "For fold : 30\n",
      "1\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}}\n",
      "The final accuracy is :  0.7027777777777777\n",
      "The average F1-score is :  0.7027777777777777\n"
     ]
    }
   ],
   "source": [
    "#LOW-MEDIUM\n",
    "\n",
    "CHO_LH_data = CHO_data[CHO_data['Carb'].isin([51,48])] #considering the two classes for binary classification\n",
    "CHO_LH_data.loc[CHO_LH_data.Carb == 51, 'CHO_bin'] = 0 #giving them binary values, as double variables sometimes create error\n",
    "CHO_LH_data.loc[CHO_LH_data.Carb== 48, 'CHO_bin'] = 1\n",
    "CHO_LH_data = CHO_LH_data.drop(['Carb'], axis=1)\n",
    "#the above 4 lines will be different for different classification, the remaining code will be same\n",
    "X = CHO_LH_data.loc[:, CHO_LH_data.columns != 'CHO_bin'] #Feature set\n",
    "Y = CHO_LH_data.loc[:, CHO_LH_data.columns == 'CHO_bin'] #target value/label\n",
    "\n",
    "values =['2133-002', '2133-022', '2133-020', '2133-004', '2133-010', '2133-015',\n",
    " '2133-040' ,'2133-013' ,'2133-008' ,'2133-019' ,'2133-001', '2133-024',\n",
    " '2133-011' ,'2133-018', '2133-006', '2133-032' ,'2133-025', '2133-017',\n",
    " '2133-028' ,'2133-039', '2133-021', '2133-030', '2133-033', '2133-037',\n",
    " '2133-026', '2133-012' ,'2133-036', '2133-035', '2133-041' ,'2133-009'] #needed for manual loss\n",
    "\n",
    "final_accuracy =[] #will be used for finding mean across nested cross validation\n",
    "F1score = []\n",
    "\n",
    "for i in range(0,30): #as we have 30 IDs\n",
    "    print(\"For fold :\", i+1) #will print the current fold number, this is for initial check, can delete it\n",
    "    test =CHO_LH_data[CHO_LH_data['TruncatedID'].isin([values[i]])] #test data    \n",
    "    train =CHO_LH_data[~CHO_LH_data['TruncatedID'].isin([values[i]])]#train data\n",
    "    X_train = train.loc[:, train.columns != 'CHO_bin'] #test feature\n",
    "    Y_train = train.loc[:, train.columns == 'CHO_bin'] #test label\n",
    "    X_test = test.loc[:, test.columns != 'CHO_bin']#train feature    \n",
    "    Y_test =test.loc[:, test.columns == 'CHO_bin']#train label\n",
    "    \n",
    "    train_ids = X_train['TruncatedID']#needed for inner cross-validation\n",
    "    train_ids = np.array(train_ids)#needed for inner cross-validation\n",
    "    train_ids = np.unique(train_ids)#needed for inner cross-validation\n",
    "    X_train = X_train.drop(['TruncatedID'],axis = 1) #dropping this column as it is not a feature for classification\n",
    "    X_test = X_test.drop(['TruncatedID'],axis = 1)#dropping this column as it is not a feature for classification\n",
    "    max_acc = 0 #initializing maximum accuracy which will be needed later    \n",
    "    ypred_all = []\n",
    "    for C_val in range(1,20): #ranges of decision tree depth\n",
    "        ypred_all = []\n",
    "        for j in range(0,29): #as inner cross validation ha 29 IDs\n",
    "            inner_test = train[train['TruncatedID'].isin([train_ids[j]])] #validation set\n",
    "            inner_train = train[~train['TruncatedID'].isin([train_ids[j]])]#inner train set\n",
    "            xtr = inner_train.loc[:, inner_train.columns != 'CHO_bin'] #inner train set features\n",
    "            ytr = inner_train.loc[:, inner_train.columns == 'CHO_bin']# inner train set label\n",
    "            xvl = inner_test.loc[:, inner_test.columns != 'CHO_bin']#validation set features\n",
    "            yvl = inner_test.loc[:, inner_test.columns == 'CHO_bin']#validation set labels\n",
    "            xtr = xtr.drop(['TruncatedID'],axis = 1)#We dont need this column for classification        \n",
    "            xvl = xvl.drop(['TruncatedID'],axis = 1)#We dont need this column for classification\n",
    "            clf = svm.SVC(C=C_val,kernel='linear') #SVM classifier\n",
    "            #convert dataframe to array\n",
    "            #xtr_arr = xtr.as_matrix(columns=None)\n",
    "            #ytr_arr = ytr.as_matrix(columns=None)\n",
    "            #clf = clf.fit(xtr_arr,ytr_arr)\n",
    "            clf=clf.fit(xtr,ytr)\n",
    "            ypred = clf.predict(xvl)#prediction\n",
    "            ypred_all.append(accuracy_score(yvl,ypred))\n",
    "        ypred_mean = np.mean(ypred_all)#finding mean of all accuracy\n",
    "        if max_acc<ypred_mean:#finding the depth which gives maximum accuracy\n",
    "            max_acc = ypred_mean\n",
    "            best_param = C_val\n",
    "    #best SVM classifier\n",
    "    print(best_param)\n",
    "    clf = svm.SVC(C=best_param,kernel='linear') \n",
    "    clf=clf.fit(X_train,Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    final_accuracy.append(accuracy_score(Y_test,Y_pred))\n",
    "    dic = classification_report(Y_test,Y_pred,output_dict=True)\n",
    "    print(dic)\n",
    "    dic2 = dic['micro avg']\n",
    "    F1score.append(dic2['f1-score'])\n",
    "print(\"The final accuracy is : \", np.mean(final_accuracy))#final result (mean of accuracies in outer CV)\n",
    "print(\"The average F1-score is : \", np.mean(F1score))#final result (mean of accuracies in outer CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
