{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahulphaniraj/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from pandas import Series, DataFrame\n",
    "from pylab import rcParams\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "sb.set(style=\"white\")\n",
    "sb.set(style=\"whitegrid\", color_codes=True)\n",
    "import itertools\n",
    "from sklearn import svm\n",
    "import graphviz\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold, LeaveOneOut\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171, 13)\n",
      "['ID', 'TruncatedID', 'Mean', 'SD', 'AUC', 'Peak', 'MaxIndex', 'Halfpk', 'Skew', 'HHMean', 'Fat', 'Carb', 'Protein']\n"
     ]
    }
   ],
   "source": [
    "### reading data and printing the data ###\n",
    "data = pd.read_csv('/home/sahulphaniraj/csce633-ml/MachineLearningProject/Dataset/classical_feature.csv',names=['ID','TruncatedID','Mean','SD','AUC','Peak','MaxIndex','Halfpk','Skew','HHMean','Fat','Carb','Protein'\n",
    "])\n",
    "print(data.shape)\n",
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.  18.   2.5]\n",
      "['2133-002' '2133-022' '2133-020' '2133-004' '2133-010' '2133-015'\n",
      " '2133-040' '2133-013' '2133-008' '2133-019' '2133-001' '2133-024'\n",
      " '2133-011' '2133-018' '2133-006' '2133-032' '2133-025' '2133-017'\n",
      " '2133-028' '2133-039' '2133-021' '2133-030' '2133-033' '2133-037'\n",
      " '2133-026' '2133-012' '2133-036' '2133-035' '2133-041' '2133-009']\n"
     ]
    }
   ],
   "source": [
    "### CHO\n",
    "CHO_data = data.drop(['ID','Protein','Carb'], axis=1) #taking only the Carb target column\n",
    "print(CHO_data.Fat.unique())# Finding the unique values in that column\n",
    "print(CHO_data.TruncatedID.unique()) #finiding unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold : 1\n",
      "14\n",
      "For fold : 2\n",
      "10\n",
      "For fold : 3\n",
      "17\n",
      "For fold : 4\n",
      "11\n",
      "For fold : 5\n",
      "10\n",
      "For fold : 6\n",
      "11\n",
      "For fold : 7\n",
      "12\n",
      "For fold : 8\n",
      "12\n",
      "For fold : 9\n",
      "13\n",
      "For fold : 10\n",
      "12\n",
      "For fold : 11\n",
      "14\n",
      "For fold : 12\n",
      "11\n",
      "For fold : 13\n",
      "14\n",
      "For fold : 14\n",
      "10\n",
      "For fold : 15\n",
      "19\n",
      "For fold : 16\n",
      "17\n",
      "For fold : 17\n",
      "15\n",
      "For fold : 18\n",
      "11\n",
      "For fold : 19\n",
      "10\n",
      "For fold : 20\n",
      "10\n",
      "For fold : 21\n",
      "11\n",
      "For fold : 22\n",
      "14\n",
      "For fold : 23\n",
      "13\n",
      "For fold : 24\n",
      "10\n",
      "For fold : 25\n",
      "13\n",
      "For fold : 26\n",
      "11\n",
      "For fold : 27\n",
      "13\n",
      "For fold : 28\n",
      "13\n",
      "For fold : 29\n",
      "13\n",
      "For fold : 30\n",
      "12\n",
      "The final accuracy is :  0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "###for LOW-HIGH\n",
    "\n",
    "CHO_LH_data = CHO_data[CHO_data['Fat'].isin([2.5,20.0])] #considering the two classes for binary classification\n",
    "CHO_LH_data.loc[CHO_LH_data.Fat == 2.5, 'CHO_bin'] = 0 #giving them binary values, as double variables sometimes create error\n",
    "CHO_LH_data.loc[CHO_LH_data.Fat== 20.0, 'CHO_bin'] = 1\n",
    "CHO_LH_data = CHO_LH_data.drop(['Fat'], axis=1)\n",
    "#the above 4 lines will be different for different classification, the remaining code will be same\n",
    "X = CHO_LH_data.loc[:, CHO_LH_data.columns != 'CHO_bin'] #Feature set\n",
    "Y = CHO_LH_data.loc[:, CHO_LH_data.columns == 'CHO_bin'] #target value/label\n",
    "\n",
    "values =['2133-002', '2133-022', '2133-020', '2133-004', '2133-010', '2133-015',\n",
    " '2133-040' ,'2133-013' ,'2133-008' ,'2133-019' ,'2133-001', '2133-024',\n",
    " '2133-011' ,'2133-018', '2133-006', '2133-032' ,'2133-025', '2133-017',\n",
    " '2133-028' ,'2133-039', '2133-021', '2133-030', '2133-033', '2133-037',\n",
    " '2133-026', '2133-012' ,'2133-036', '2133-035', '2133-041' ,'2133-009'] #needed for manual loss\n",
    "\n",
    "final_accuracy =[] #will be used for finding mean across nested cross validation\n",
    "\n",
    "for i in range(0,30): #as we have 30 IDs\n",
    "    print(\"For fold :\", i+1) #will print the current fold number, this is for initial check, can delete it\n",
    "    test =CHO_LH_data[CHO_LH_data['TruncatedID'].isin([values[i]])] #test data    \n",
    "    train =CHO_LH_data[~CHO_LH_data['TruncatedID'].isin([values[i]])]#train data\n",
    "    X_train = train.loc[:, train.columns != 'CHO_bin'] #test feature\n",
    "    Y_train = train.loc[:, train.columns == 'CHO_bin'] #test label\n",
    "    X_test = test.loc[:, test.columns != 'CHO_bin']#train feature    \n",
    "    Y_test =test.loc[:, test.columns == 'CHO_bin']#train label\n",
    "    \n",
    "    train_ids = X_train['TruncatedID']#needed for inner cross-validation\n",
    "    train_ids = np.array(train_ids)#needed for inner cross-validation\n",
    "    train_ids = np.unique(train_ids)#needed for inner cross-validation\n",
    "    X_train = X_train.drop(['TruncatedID'],axis = 1) #dropping this column as it is not a feature for classification\n",
    "    X_test = X_test.drop(['TruncatedID'],axis = 1)#dropping this column as it is not a feature for classification\n",
    "    max_acc = 0 #initializing maximum accuracy which will be needed later    \n",
    "    ypred_all = []\n",
    "    for C_val in range(1,20): #ranges of decision tree depth\n",
    "        ypred_all = []\n",
    "        for j in range(0,29): #as inner cross validation ha 29 IDs\n",
    "            inner_test = train[train['TruncatedID'].isin([train_ids[j]])] #validation set\n",
    "            inner_train = train[~train['TruncatedID'].isin([train_ids[j]])]#inner train set\n",
    "            xtr = inner_train.loc[:, inner_train.columns != 'CHO_bin'] #inner train set features\n",
    "            ytr = inner_train.loc[:, inner_train.columns == 'CHO_bin']# inner train set label\n",
    "            xvl = inner_test.loc[:, inner_test.columns != 'CHO_bin']#validation set features\n",
    "            yvl = inner_test.loc[:, inner_test.columns == 'CHO_bin']#validation set labels\n",
    "            xtr = xtr.drop(['TruncatedID'],axis = 1)#We dont need this column for classification        \n",
    "            xvl = xvl.drop(['TruncatedID'],axis = 1)#We dont need this column for classification\n",
    "            clf = svm.SVC(C=C_val) #SVM classifier\n",
    "            #convert dataframe to array\n",
    "            #xtr_arr = xtr.as_matrix(columns=None)\n",
    "            #ytr_arr = ytr.as_matrix(columns=None)\n",
    "            #clf = clf.fit(xtr_arr,ytr_arr)\n",
    "            clf=clf.fit(xtr,ytr)\n",
    "            ypred = clf.predict(xvl)#prediction\n",
    "            ypred_all.append(accuracy_score(yvl,ypred))\n",
    "        ypred_mean = np.mean(ypred_all)#finding mean of all accuracy\n",
    "        if max_acc<ypred_mean:#finding the depth which gives maximum accuracy\n",
    "            max_acc = ypred_mean\n",
    "            best_param = C_val\n",
    "    #best SVM classifier\n",
    "    print(best_param)\n",
    "    clf = svm.SVC(C=best_param) \n",
    "    clf=clf.fit(X_train,Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    final_accuracy.append(accuracy_score(Y_test,Y_pred))\n",
    "print(\"The final accuracy is : \", np.mean(final_accuracy))#final result (mean of accuracies in outer CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold : 1\n",
      "5\n",
      "For fold : 2\n",
      "6\n",
      "For fold : 3\n",
      "4\n",
      "For fold : 4\n",
      "8\n",
      "For fold : 5\n",
      "5\n",
      "For fold : 6\n",
      "4\n",
      "For fold : 7\n",
      "4\n",
      "For fold : 8\n",
      "4\n",
      "For fold : 9\n",
      "4\n",
      "For fold : 10\n",
      "4\n",
      "For fold : 11\n",
      "4\n",
      "For fold : 12\n",
      "10\n",
      "For fold : 13\n",
      "4\n",
      "For fold : 14\n",
      "4\n",
      "For fold : 15\n",
      "5\n",
      "For fold : 16\n",
      "5\n",
      "For fold : 17\n",
      "4\n",
      "For fold : 18\n",
      "17\n",
      "For fold : 19\n",
      "4\n",
      "For fold : 20\n",
      "3\n",
      "For fold : 21\n",
      "3\n",
      "For fold : 22\n",
      "4\n",
      "For fold : 23\n",
      "5\n",
      "For fold : 24\n",
      "4\n",
      "For fold : 25\n",
      "4\n",
      "For fold : 26\n",
      "12\n",
      "For fold : 27\n",
      "4\n",
      "For fold : 28\n",
      "12\n",
      "For fold : 29\n",
      "5\n",
      "For fold : 30\n",
      "4\n",
      "The final accuracy is :  0.6777777777777777\n"
     ]
    }
   ],
   "source": [
    "#MEDIUM-HIGH\n",
    "\n",
    "CHO_LH_data = CHO_data[CHO_data['Fat'].isin([18.0,20.0])] #considering the two classes for binary classification\n",
    "CHO_LH_data.loc[CHO_LH_data.Fat == 18.0, 'CHO_bin'] = 0 #giving them binary values, as double variables sometimes create error\n",
    "CHO_LH_data.loc[CHO_LH_data.Fat== 20.0, 'CHO_bin'] = 1\n",
    "CHO_LH_data = CHO_LH_data.drop(['Fat'], axis=1)\n",
    "#the above 4 lines will be different for different classification, the remaining code will be same\n",
    "X = CHO_LH_data.loc[:, CHO_LH_data.columns != 'CHO_bin'] #Feature set\n",
    "Y = CHO_LH_data.loc[:, CHO_LH_data.columns == 'CHO_bin'] #target value/label\n",
    "\n",
    "values =['2133-002', '2133-022', '2133-020', '2133-004', '2133-010', '2133-015',\n",
    " '2133-040' ,'2133-013' ,'2133-008' ,'2133-019' ,'2133-001', '2133-024',\n",
    " '2133-011' ,'2133-018', '2133-006', '2133-032' ,'2133-025', '2133-017',\n",
    " '2133-028' ,'2133-039', '2133-021', '2133-030', '2133-033', '2133-037',\n",
    " '2133-026', '2133-012' ,'2133-036', '2133-035', '2133-041' ,'2133-009'] #needed for manual loss\n",
    "\n",
    "final_accuracy =[] #will be used for finding mean across nested cross validation\n",
    "\n",
    "for i in range(0,30): #as we have 30 IDs\n",
    "    print(\"For fold :\", i+1) #will print the current fold number, this is for initial check, can delete it\n",
    "    test =CHO_LH_data[CHO_LH_data['TruncatedID'].isin([values[i]])] #test data    \n",
    "    train =CHO_LH_data[~CHO_LH_data['TruncatedID'].isin([values[i]])]#train data\n",
    "    X_train = train.loc[:, train.columns != 'CHO_bin'] #test feature\n",
    "    Y_train = train.loc[:, train.columns == 'CHO_bin'] #test label\n",
    "    X_test = test.loc[:, test.columns != 'CHO_bin']#train feature    \n",
    "    Y_test =test.loc[:, test.columns == 'CHO_bin']#train label\n",
    "    \n",
    "    train_ids = X_train['TruncatedID']#needed for inner cross-validation\n",
    "    train_ids = np.array(train_ids)#needed for inner cross-validation\n",
    "    train_ids = np.unique(train_ids)#needed for inner cross-validation\n",
    "    X_train = X_train.drop(['TruncatedID'],axis = 1) #dropping this column as it is not a feature for classification\n",
    "    X_test = X_test.drop(['TruncatedID'],axis = 1)#dropping this column as it is not a feature for classification\n",
    "    max_acc = 0 #initializing maximum accuracy which will be needed later    \n",
    "    ypred_all = []\n",
    "    for C_val in range(1,20): #ranges of decision tree depth\n",
    "        ypred_all = []\n",
    "        for j in range(0,29): #as inner cross validation ha 29 IDs\n",
    "            inner_test = train[train['TruncatedID'].isin([train_ids[j]])] #validation set\n",
    "            inner_train = train[~train['TruncatedID'].isin([train_ids[j]])]#inner train set\n",
    "            xtr = inner_train.loc[:, inner_train.columns != 'CHO_bin'] #inner train set features\n",
    "            ytr = inner_train.loc[:, inner_train.columns == 'CHO_bin']# inner train set label\n",
    "            xvl = inner_test.loc[:, inner_test.columns != 'CHO_bin']#validation set features\n",
    "            yvl = inner_test.loc[:, inner_test.columns == 'CHO_bin']#validation set labels\n",
    "            xtr = xtr.drop(['TruncatedID'],axis = 1)#We dont need this column for classification        \n",
    "            xvl = xvl.drop(['TruncatedID'],axis = 1)#We dont need this column for classification\n",
    "            clf = svm.SVC(C=C_val) #SVM classifier\n",
    "            #convert dataframe to array\n",
    "            #xtr_arr = xtr.as_matrix(columns=None)\n",
    "            #ytr_arr = ytr.as_matrix(columns=None)\n",
    "            #clf = clf.fit(xtr_arr,ytr_arr)\n",
    "            clf=clf.fit(xtr,ytr)\n",
    "            ypred = clf.predict(xvl)#prediction\n",
    "            ypred_all.append(accuracy_score(yvl,ypred))\n",
    "        ypred_mean = np.mean(ypred_all)#finding mean of all accuracy\n",
    "        if max_acc<ypred_mean:#finding the depth which gives maximum accuracy\n",
    "            max_acc = ypred_mean\n",
    "            best_param = C_val\n",
    "    #best SVM classifier\n",
    "    print(best_param)\n",
    "    clf = svm.SVC(C=best_param) \n",
    "    clf=clf.fit(X_train,Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    final_accuracy.append(accuracy_score(Y_test,Y_pred))\n",
    "print(\"The final accuracy is : \", np.mean(final_accuracy))#final result (mean of accuracies in outer CV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fold : 1\n",
      "4\n",
      "For fold : 2\n",
      "3\n",
      "For fold : 3\n",
      "5\n",
      "For fold : 4\n",
      "11\n",
      "For fold : 5\n",
      "17\n",
      "For fold : 6\n",
      "4\n",
      "For fold : 7\n",
      "3\n",
      "For fold : 8\n",
      "5\n",
      "For fold : 9\n",
      "4\n",
      "For fold : 10\n",
      "16\n",
      "For fold : 11\n",
      "4\n",
      "For fold : 12\n",
      "9\n",
      "For fold : 13\n",
      "4\n",
      "For fold : 14\n",
      "12\n",
      "For fold : 15\n",
      "5\n",
      "For fold : 16\n",
      "4\n",
      "For fold : 17\n",
      "3\n",
      "For fold : 18\n",
      "4\n",
      "For fold : 19\n",
      "5\n",
      "For fold : 20\n",
      "17\n",
      "For fold : 21\n",
      "3\n",
      "For fold : 22\n",
      "4\n",
      "For fold : 23\n",
      "4\n",
      "For fold : 24\n",
      "5\n",
      "For fold : 25\n",
      "4\n",
      "For fold : 26\n",
      "4\n",
      "For fold : 27\n",
      "4\n",
      "For fold : 28\n",
      "4\n",
      "For fold : 29\n",
      "4\n",
      "For fold : 30\n",
      "4\n",
      "The final accuracy is :  0.65\n"
     ]
    }
   ],
   "source": [
    "#LOW-MEDIUM\n",
    "\n",
    "CHO_LH_data = CHO_data[CHO_data['Fat'].isin([2.5,18.0])] #considering the two classes for binary classification\n",
    "CHO_LH_data.loc[CHO_LH_data.Fat == 2.5, 'CHO_bin'] = 0 #giving them binary values, as double variables sometimes create error\n",
    "CHO_LH_data.loc[CHO_LH_data.Fat== 18.0, 'CHO_bin'] = 1\n",
    "CHO_LH_data = CHO_LH_data.drop(['Fat'], axis=1)\n",
    "#the above 4 lines will be different for different classification, the remaining code will be same\n",
    "X = CHO_LH_data.loc[:, CHO_LH_data.columns != 'CHO_bin'] #Feature set\n",
    "Y = CHO_LH_data.loc[:, CHO_LH_data.columns == 'CHO_bin'] #target value/label\n",
    "\n",
    "values =['2133-002', '2133-022', '2133-020', '2133-004', '2133-010', '2133-015',\n",
    " '2133-040' ,'2133-013' ,'2133-008' ,'2133-019' ,'2133-001', '2133-024',\n",
    " '2133-011' ,'2133-018', '2133-006', '2133-032' ,'2133-025', '2133-017',\n",
    " '2133-028' ,'2133-039', '2133-021', '2133-030', '2133-033', '2133-037',\n",
    " '2133-026', '2133-012' ,'2133-036', '2133-035', '2133-041' ,'2133-009'] #needed for manual loss\n",
    "\n",
    "final_accuracy =[] #will be used for finding mean across nested cross validation\n",
    "\n",
    "for i in range(0,30): #as we have 30 IDs\n",
    "    print(\"For fold :\", i+1) #will print the current fold number, this is for initial check, can delete it\n",
    "    test =CHO_LH_data[CHO_LH_data['TruncatedID'].isin([values[i]])] #test data    \n",
    "    train =CHO_LH_data[~CHO_LH_data['TruncatedID'].isin([values[i]])]#train data\n",
    "    X_train = train.loc[:, train.columns != 'CHO_bin'] #test feature\n",
    "    Y_train = train.loc[:, train.columns == 'CHO_bin'] #test label\n",
    "    X_test = test.loc[:, test.columns != 'CHO_bin']#train feature    \n",
    "    Y_test =test.loc[:, test.columns == 'CHO_bin']#train label\n",
    "    \n",
    "    train_ids = X_train['TruncatedID']#needed for inner cross-validation\n",
    "    train_ids = np.array(train_ids)#needed for inner cross-validation\n",
    "    train_ids = np.unique(train_ids)#needed for inner cross-validation\n",
    "    X_train = X_train.drop(['TruncatedID'],axis = 1) #dropping this column as it is not a feature for classification\n",
    "    X_test = X_test.drop(['TruncatedID'],axis = 1)#dropping this column as it is not a feature for classification\n",
    "    max_acc = 0 #initializing maximum accuracy which will be needed later    \n",
    "    ypred_all = []\n",
    "    for C_val in range(1,20): #ranges of decision tree depth\n",
    "        ypred_all = []\n",
    "        for j in range(0,29): #as inner cross validation ha 29 IDs\n",
    "            inner_test = train[train['TruncatedID'].isin([train_ids[j]])] #validation set\n",
    "            inner_train = train[~train['TruncatedID'].isin([train_ids[j]])]#inner train set\n",
    "            xtr = inner_train.loc[:, inner_train.columns != 'CHO_bin'] #inner train set features\n",
    "            ytr = inner_train.loc[:, inner_train.columns == 'CHO_bin']# inner train set label\n",
    "            xvl = inner_test.loc[:, inner_test.columns != 'CHO_bin']#validation set features\n",
    "            yvl = inner_test.loc[:, inner_test.columns == 'CHO_bin']#validation set labels\n",
    "            xtr = xtr.drop(['TruncatedID'],axis = 1)#We dont need this column for classification        \n",
    "            xvl = xvl.drop(['TruncatedID'],axis = 1)#We dont need this column for classification\n",
    "            clf = svm.SVC(C=C_val) #SVM classifier\n",
    "            #convert dataframe to array\n",
    "            #xtr_arr = xtr.as_matrix(columns=None)\n",
    "            #ytr_arr = ytr.as_matrix(columns=None)\n",
    "            #clf = clf.fit(xtr_arr,ytr_arr)\n",
    "            clf=clf.fit(xtr,ytr)\n",
    "            ypred = clf.predict(xvl)#prediction\n",
    "            ypred_all.append(accuracy_score(yvl,ypred))\n",
    "        ypred_mean = np.mean(ypred_all)#finding mean of all accuracy\n",
    "        if max_acc<ypred_mean:#finding the depth which gives maximum accuracy\n",
    "            max_acc = ypred_mean\n",
    "            best_param = C_val\n",
    "    #best SVM classifier\n",
    "    print(best_param)\n",
    "    clf = svm.SVC(C=best_param) \n",
    "    clf=clf.fit(X_train,Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    final_accuracy.append(accuracy_score(Y_test,Y_pred))\n",
    "print(\"The final accuracy is : \", np.mean(final_accuracy))#final result (mean of accuracies in outer CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
